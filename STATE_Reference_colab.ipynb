{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0afc135f-de38-4418-8817-a011ec92b8f0",
   "metadata": {},
   "source": [
    "# Repeat from colab jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e23be7aa-6a31-4db8-8251-2e7109b9e4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: state [-h] {emb,tx} ...\n",
      "\n",
      "positional arguments:\n",
      "  {emb,tx}\n",
      "\n",
      "options:\n",
      "  -h, --help  show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "# Dockerfile has state installed\n",
    "# Looking at your Dockerfile, the issue is that uv tool install creates an isolated environment that may not persist properly in the Docker container\n",
    "# cause wandb config files missing \n",
    "! state --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1bd34d-7cc2-43eb-bc54-cdaaa6eb2797",
   "metadata": {},
   "source": [
    "# Clone the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2078c65a-cdd8-4d15-b957-2c9243586e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'state'...\n",
      "remote: Enumerating objects: 6345, done.\u001b[K\n",
      "remote: Counting objects: 100% (505/505), done.\u001b[K\n",
      "remote: Compressing objects: 100% (209/209), done.\u001b[K90/209)\u001b[K\n",
      "remote: Total 6345 (delta 367), reused 345 (delta 274), pack-reused 5840 (from 2)\u001b[K\n",
      "Receiving objects: 100% (6345/6345), 122.69 MiB | 17.25 MiB/s, done.\n",
      "Resolving deltas: 100% (3866/3866), done.\n",
      "/workspace/state\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository\n",
    "! git clone https://github.com/ArcInstitute/state.git\n",
    "%cd state\n",
    "\n",
    "# Colab-specific config for pytorch lightning\n",
    "import os\n",
    "os.environ['MPLBACKEND'] = 'Agg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9bb1c-9f4f-4e9e-86f7-e6f34c02bc04",
   "metadata": {},
   "source": [
    "# Download the Replogle-Nadig training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d370037-ce93-4a29-a0a9-a889298d2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir training_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ab8eaca-6f08-4b5f-aa85-f7e976600d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (4.66.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c2cfa848ed4b659a43b73df44d0e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install requests tqdm\n",
    "\n",
    "import requests\n",
    "from tqdm.auto import tqdm  # picks the best bar for the environment\n",
    "\n",
    "url = \"https://storage.googleapis.com/vcc_data_prod/datasets/state/competition_support_set.zip\"\n",
    "output_path = \"/workspace/training_dataset/competition_support_set.zip\"\n",
    "\n",
    "# stream the download so we can track progress\n",
    "response = requests.get(url, stream=True)\n",
    "total = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "with open(output_path, \"wb\") as f, tqdm(\n",
    "    total=total, unit='B', unit_scale=True, desc=\"Downloading\"\n",
    ") as bar:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        if not chunk:\n",
    "            break\n",
    "        f.write(chunk)\n",
    "        bar.update(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75cb0b58-fbbe-46d4-852d-75d2e67642dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fed423a771740b38cfc774236624125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unzipping:   0%|          | 0/10 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "out_dir  = \"/workspace/training_dataset/competition_support_set\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with ZipFile(output_path, 'r') as z:\n",
    "    for member in tqdm(z.infolist(), desc=\"Unzipping\", unit=\"file\"):\n",
    "        z.extract(member, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c28876a-1732-487e-afaf-10b335796879",
   "metadata": {},
   "source": [
    "# Set Weights and Biases Entity for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24c825a7-ed32-418c-b5e7-65fcefffcae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set Weights and Biases Entity for tracking\n",
    "entity = \"arcinstitute\" # @param {\"type\":\"string\",\"placeholder\":\"arcinstitute\"}\n",
    "! sed -i 's|entity: your_entity_name|entity: ${entity}|g' state/src/state/configs/wandb/default.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07a0ab-b7dc-4240-85bb-34c50473d31b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35a7e44-f3e9-42f1-8deb-145c70e14716",
   "metadata": {},
   "source": [
    "### State TX Training Command\n",
    "# This setups up training for State across datasets, using ESM2 featurizations\n",
    "# of genes as the perturbation embeddings. Note that we are now generalizing\n",
    "# across both contexts and perturbations (not just contexts)\n",
    "! state tx train \\\n",
    "  data.kwargs.toml_config_path=\"/workspace/training_dataset/competition_support_set/starter.toml\" \\\n",
    "  data.kwargs.num_workers=4 \\\n",
    "  data.kwargs.batch_col=\"batch_var\" \\\n",
    "  data.kwargs.pert_col=\"target_gene\" \\\n",
    "  data.kwargs.cell_type_key=\"cell_type\" \\\n",
    "  data.kwargs.control_pert=\"non-targeting\" \\\n",
    "  data.kwargs.perturbation_features_file=\"/workspace/training_dataset/competition_support_set/ESM2_pert_features.pt\" \\\n",
    "  training.max_steps=400 \\\n",
    "  training.ckpt_every_n_steps=200 \\\n",
    "  model=state_sm \\\n",
    "  wandb.tags=\"[first_run]\" \\\n",
    "  output_dir=\"/workspace/colab_competition/\" \\\n",
    "  name=\"first_run\" data.kwargs.embed_key=null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a40e9c67-18cd-437a-a362-6261bdcd9e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/workspace/training_dataset/competition_support_set/{competition_train,k562_gwps,rpe1,jurkat,k562,hepg2}.h5\n",
      "Dataset path does not exist: /workspace/training_dataset/competition_support_set/{competition_train,k562_gwps,rpe1,jurkat,k562,hepg2}.h5\n",
      "\n",
      "Processing replogle_h1:   0%|          | 0/6 [00:00<?, ?it/s]\n",
      "                                                             \n",
      "Processed competition_train: 221273 train, 0 val, 0 test\n",
      "\n",
      "Processing replogle_h1:   0%|          | 0/6 [00:00<?, ?it/s]\n",
      "Processing replogle_h1:  17%|█▋        | 1/6 [00:00<00:00,  5.01it/s]No cell barcode information found in /workspace/training_dataset/competition_support_set/k562_gwps.h5. Generating generic barcodes.\n",
      "\n",
      "                                                                     \n",
      "Processed k562_gwps: 111605 train, 0 val, 0 test\n",
      "\n",
      "Processing replogle_h1:  17%|█▋        | 1/6 [00:00<00:00,  5.01it/s]No cell barcode information found in /workspace/training_dataset/competition_support_set/rpe1.h5. Generating generic barcodes.\n",
      "\n",
      "                                                                     \n",
      "Processed rpe1: 22317 train, 0 val, 0 test\n",
      "\n",
      "Processing replogle_h1:  17%|█▋        | 1/6 [00:00<00:00,  5.01it/s]No cell barcode information found in /workspace/training_dataset/competition_support_set/jurkat.h5. Generating generic barcodes.\n",
      "\n",
      "                                                                     \n",
      "Processed jurkat: 21412 train, 0 val, 0 test\n",
      "\n",
      "Processing replogle_h1:  17%|█▋        | 1/6 [00:00<00:00,  5.01it/s]No cell barcode information found in /workspace/training_dataset/competition_support_set/k562.h5. Generating generic barcodes.\n",
      "\n",
      "                                                                     \n",
      "Processed k562: 18465 train, 0 val, 0 test\n",
      "\n",
      "Processing replogle_h1:  17%|█▋        | 1/6 [00:00<00:00,  5.01it/s]No cell barcode information found in /workspace/training_dataset/competition_support_set/hepg2.h5. Generating generic barcodes.\n",
      "\n",
      "                                                                     \n",
      "Processed hepg2: 0 train, 0 val, 9386 test\n",
      "\n",
      "Processing replogle_h1:  17%|█▋        | 1/6 [00:00<00:00,  5.01it/s]\n",
      "Processing replogle_h1: 100%|██████████| 6/6 [00:00<00:00, 20.73it/s]\n",
      "num_workers: 4\n",
      "batch size: None\n",
      "PertSetsPerturbationModel(\n",
      "  (loss_fn): SamplesLoss()\n",
      "  (pert_encoder): Sequential(\n",
      "    (0): Linear(in_features=5120, out_features=672, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=672, out_features=672, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=672, out_features=672, bias=True)\n",
      "    (7): GELU(approximate='none')\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=672, out_features=672, bias=True)\n",
      "  )\n",
      "  (basal_encoder): Linear(in_features=18080, out_features=672, bias=True)\n",
      "  (transformer_backbone): LlamaBidirectionalModel(\n",
      "    (embed_tokens): Embedding(32000, 672, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=672, out_features=672, bias=False)\n",
      "          (k_proj): Linear(in_features=672, out_features=672, bias=False)\n",
      "          (v_proj): Linear(in_features=672, out_features=672, bias=False)\n",
      "          (o_proj): Linear(in_features=672, out_features=672, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=672, out_features=2688, bias=False)\n",
      "          (up_proj): Linear(in_features=672, out_features=2688, bias=False)\n",
      "          (down_proj): Linear(in_features=2688, out_features=672, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((672,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((672,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((672,), eps=1e-06)\n",
      "    (rotary_emb): NoRoPE()\n",
      "  )\n",
      "  (project_out): Sequential(\n",
      "    (0): Linear(in_features=672, out_features=672, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=672, out_features=672, bias=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=672, out_features=672, bias=True)\n",
      "    (7): GELU(approximate='none')\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=672, out_features=18080, bias=True)\n",
      "  )\n",
      "  (final_down_then_up): Sequential(\n",
      "    (0): Linear(in_features=18080, out_features=2260, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=2260, out_features=18080, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Model created. Estimated params size: 0.61 GB\n",
      "Warning: Failed to initialize wandb logger: api_key not configured (no-tty). call wandb.login(key=[your_api_key])\n",
      "Continuing without wandb logging.\n",
      "Building trainer with kwargs: {'accelerator': 'cpu', 'devices': 1, 'max_steps': 400, 'check_val_every_n_epoch': None, 'val_check_interval': 2000, 'logger': [<state.tx.utils.RobustCSVLogger object at 0xffff33842510>], 'plugins': [], 'callbacks': [<lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint object at 0xffff343e1c50>, <lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint object at 0xffff32c32410>, <state.tx.callbacks.batch_speed_monitor.BatchSpeedMonitorCallback object at 0xffff32849790>], 'gradient_clip_val': 10}\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer built successfully\n",
      "Model device: cpu\n",
      "CUDA memory allocated: 0.00 GB\n",
      "CUDA memory reserved: 0.00 GB\n",
      "About to call trainer.fit() with checkpoint_path=None\n",
      "\n",
      "  | Name                 | Type                    | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | loss_fn              | SamplesLoss             | 0      | train\n",
      "1 | pert_encoder         | Sequential              | 4.8 M  | train\n",
      "2 | basal_encoder        | Linear                  | 12.2 M | train\n",
      "3 | transformer_backbone | LlamaBidirectionalModel | 50.4 M | train\n",
      "4 | project_out          | Sequential              | 13.5 M | train\n",
      "5 | final_down_then_up   | Sequential              | 81.7 M | train\n",
      "6 | relu                 | ReLU                    | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "141 M     Trainable params\n",
      "21.5 M    Non-trainable params\n",
      "162 M     Total params\n",
      "650.506   Total estimated model params size (MB)\n",
      "86        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000Traceback (most recent call last):\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1284, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/queues.py\", line 113, in get\n",
      "    if not self._poll(timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/connection.py\", line 440, in _poll\n",
      "    r = wait([self], timeout)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 1889) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.local/bin/state\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/workspace/state/src/state/__main__.py\", line 62, in main\n",
      "    run_tx_train(cfg)\n",
      "  File \"/workspace/state/src/state/_cli/_tx/_train.py\", line 342, in run_tx_train\n",
      "    trainer.fit(\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 138, in run\n",
      "    batch, batch_idx, dataloader_idx = next(data_fetcher)\n",
      "                                       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py\", line 134, in __next__\n",
      "    batch = super().__next__()\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py\", line 61, in __next__\n",
      "    batch = next(self.iterator)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 341, in __next__\n",
      "    out = next(self._iterator)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 142, in __next__\n",
      "    out = next(self.iterators[0])\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 733, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1491, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1453, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1297, in _try_get_data\n",
      "    raise RuntimeError(\n",
      "RuntimeError: DataLoader worker (pid(s) 1889) exited unexpectedly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to solve the interactive input 3 for wandb\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "env = os.environ.copy()\n",
    "env['MPLBACKEND'] = 'Agg'\n",
    "env['PYTHONPATH'] = 'state/src:' + env.get('PYTHONPATH', '')\n",
    "\n",
    "# Run with input\n",
    "process = subprocess.Popen([\n",
    "    'state', 'tx', 'train',\n",
    "    'data.kwargs.toml_config_path=/workspace/training_dataset/competition_support_set/starter.toml',\n",
    "    'data.kwargs.num_workers=4',\n",
    "    'data.kwargs.batch_col=batch_var',\n",
    "    'data.kwargs.pert_col=target_gene',\n",
    "    'data.kwargs.cell_type_key=cell_type',\n",
    "    'data.kwargs.control_pert=non-targeting',\n",
    "    'data.kwargs.perturbation_features_file=/workspace/training_dataset/competition_support_set/ESM2_pert_features.pt',\n",
    "    'data.kwargs.embed_key=null',\n",
    "    'training.max_steps=400',\n",
    "    'training.ckpt_every_n_steps=200',\n",
    "    'model=state_sm',\n",
    "    'wandb.tags=[first_run]',\n",
    "    'output_dir=/workspace/colab_competition/',\n",
    "    'name=first_run'\n",
    "], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "   text=True, env=env)\n",
    "\n",
    "# Send \"3\" as input\n",
    "output, _ = process.communicate(input=\"3\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0ae8c874-d46a-4b73-8b75-b3a11ef5c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'h5py._hl.files.File'>\n",
      "Keys in root: ['X', 'layers', 'obs', 'obsm', 'obsp', 'uns', 'var', 'varm', 'varp']\n",
      "Keys in obsm: []\n",
      "Keys in X: ['data', 'indices', 'indptr']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Check the structure of one of your H5 files\n",
    "with h5py.File('/workspace/training_dataset/competition_support_set/competition_train.h5', 'r') as f:\n",
    "    print(type(f))\n",
    "    print(\"Keys in root:\", list(f.keys()))\n",
    "    if 'obsm' in f:\n",
    "        print(\"Keys in obsm:\", list(f['obsm'].keys()))\n",
    "        print(\"Keys in X:\", list(f['X'].keys()))\n",
    "    else:\n",
    "        print(\"No 'obsm' group found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "88f5cc99-9e20-442d-9862-854b73e5ca70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': <h5py.h5g.GroupID object at 0xffff82139710>}\n"
     ]
    }
   ],
   "source": [
    "# input data structure exploring\n",
    "with h5py.File('/workspace/training_dataset/competition_support_set/competition_train.h5', 'r') as f:\n",
    "    X_data = f['X']\n",
    "    print(X_data.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552aba30-97eb-4e9f-ba60-7b26eb579df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b930778-dc63-46ef-83db-bfb11699f7c9",
   "metadata": {},
   "source": [
    "### 1. Sovle wandb config missing error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0969dc1d-9689-407b-92a0-74facd3eca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  data\t   model\t\ttraining\n",
      "config.yaml  default.yaml  state-defaults.yaml\n"
     ]
    }
   ],
   "source": [
    "# path that hydra used that doesn't contain folder wandb\n",
    "! ls /root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/state/configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1e6ec6f-c98f-46bf-9641-0aca7c60b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Generic wandb configuration\n",
      "# Users should customize these values for their own use\n",
      "entity: $arcinstitute\n",
      "project: state\n",
      "local_wandb_dir: ./wandb_logs\n",
      "tags: [] \n"
     ]
    }
   ],
   "source": [
    "! cat  state/src/state/configs/wandb/default.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27bf8ed4-733a-4c90-92da-4adf8cf6157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir /root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/state/configs/wandb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bdcbd7a1-8b47-403e-b865-d50829688dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb default solved\n",
    "! cp state/src/state/configs/wandb/default.yaml /root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/state/configs/wandb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5278280-5f93-4fce-8501-9891b26b197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py\t       config_search_path_impl.py  hydra.py\n",
      "__pycache__\t       core_plugins\t\t   instantiate\n",
      "callbacks.py\t       defaults_list.py\t\t   sources_registry.py\n",
      "config_loader_impl.py  deprecation_warning.py\t   utils.py\n",
      "config_repository.py   grammar\n"
     ]
    }
   ],
   "source": [
    "! ls /root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/hydra/_internal/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db8871-f7a0-4499-b4ce-47c54b3a6e15",
   "metadata": {},
   "source": [
    "### 2. Sovle model not found error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7231ba7-7cf7-4c4b-b145-edde2ce9c6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cellcontextmean.yaml\t      replogle_llama_9657984.yaml\n",
      "celltypemean.yaml\t      replogle_llama_speculative.yaml\n",
      "cpa.yaml\t\t      scgpt-chemical.yaml\n",
      "decoder_only.yaml\t      scgpt-genetic.yaml\n",
      "embedsum.yaml\t\t      scvi.yaml\n",
      "globalsimplesum.yaml\t      simplesum.yaml\n",
      "old_neuralot.yaml\t      tahoe_best.yaml\n",
      "pertsets.yaml\t\t      tahoe_decoder_test.yaml\n",
      "replogle_best.yaml\t      tahoe_llama_156142032.yaml\n",
      "replogle_gpt_11538696.yaml    tahoe_llama_195177264.yaml\n",
      "replogle_gpt_12872384.yaml    tahoe_llama_212693232.yaml\n",
      "replogle_gpt_15522108.yaml    tahoe_llama_289720032.yaml\n",
      "replogle_gpt_28942368.yaml    tahoe_llama_30335984.yaml\n",
      "replogle_gpt_31043724.yaml    tahoe_llama_31911552.yaml\n",
      "replogle_gpt_5769512.yaml     tahoe_llama_46577712.yaml\n",
      "replogle_llama_11645640.yaml  tahoe_llama_58562784.yaml\n",
      "replogle_llama_21712320.yaml  tahoe_llama_60671280.yaml\n",
      "replogle_llama_23290296.yaml  tahoe_llama_62089464.yaml\n",
      "replogle_llama_4424880.yaml   tahoe_llama_93133848.yaml\n",
      "replogle_llama_8849104.yaml\n"
     ]
    }
   ],
   "source": [
    "! ls /root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/state/configs/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e71ce3e-507c-4553-840b-fe09b285ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp state/src/state/configs/model/state_sm.yaml /root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/state/configs/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4021bd7-6e83-4ada-82f0-b12e6213e898",
   "metadata": {},
   "source": [
    "### 3. Solve the matplotlib backend issue. \n",
    "The problem is that Jupyter Lab sets the matplotlib backend to 'module://matplotlib_inline.backend_inline', but when you're running the command in a Docker container without a display, this backend isn't valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da6d36e8-acf4-4226-bb03-8ace45e7eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MPLBACKEND'] = 'Agg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d51fa49-925d-43a5-b5e5-7413c5ffbc32",
   "metadata": {},
   "source": [
    "### 4. Training dataset path correction\n",
    "\n",
    " Dataset path does not exist: /content/state/competition_support_set/{competition_train,k562_gwps,rpe1,jurkat,k562,hepg2}.h5\n",
    " cat training_dataset/competition_support_set/starter.toml \n",
    " Dataset paths - maps dataset names to their directories\n",
    "\n",
    "replogle_h1 = \"/content/state/competition_support_set/{competition_train,k562_gwps,rpe1,jurkat,k562,hepg2}.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0e6e4-6d33-45e5-a248-ae73c8649ce2",
   "metadata": {},
   "source": [
    "### 5. trying to access an obsm key (likely an embedding key) that doesn't exist in your H5 files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8562b2e9-b43c-4a54-a81c-967470b28a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in root: ['X', 'layers', 'obs', 'obsm', 'obsp', 'uns', 'var', 'varm', 'varp']\n",
      "Keys in obsm: []\n"
     ]
    }
   ],
   "source": [
    "# no key in obsm \n",
    "import h5py\n",
    "\n",
    "# Check the structure of one of your H5 files\n",
    "with h5py.File('/workspace/training_dataset/competition_support_set/competition_train.h5', 'r') as f:\n",
    "    print(\"Keys in root:\", list(f.keys()))\n",
    "    if 'obsm' in f:\n",
    "        print(\"Keys in obsm:\", list(f['obsm'].keys()))\n",
    "    else:\n",
    "        print(\"No 'obsm' group found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa22d06e-8efb-4745-a40b-497cdbc5ad0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035630b8-1bc6-431d-8032-51e1c7c19fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb7ee7-0eab-48dc-93a3-64032b88451c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ebb7828-a9eb-4d39-80f0-7e9121133590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: state [-h] {emb,tx} ...\n",
      "\n",
      "positional arguments:\n",
      "  {emb,tx}\n",
      "\n",
      "options:\n",
      "  -h, --help  show this help message and exit\n"
     ]
    }
   ],
   "source": [
    "# to make sure the state configue files wandb exist, reinstall the sate\n",
    "# solve error hydra.errors.MissingConfigException: In 'config': Could not find 'wandb/default'\n",
    "! uv run state --help --link-mode=copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01c8eca6-2803-409d-957d-4e596cb7982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.local/bin/state\n"
     ]
    }
   ],
   "source": [
    "! ls /root/.local/bin/state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32817102-4360-4ac3-854c-ebdee993658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "!  pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20caccc-93f4-40bb-9aae-c3b1a78977f0",
   "metadata": {},
   "source": [
    "# Run inference on the competition validation perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "590c3ef0-ff65-4119-b874-2f1ef967fb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/workspace/colab_competition/first_run/checkpoints/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# View the available checkpoints\n",
    "# This will be populated as you run training\n",
    "\n",
    "! ls /workspace/colab_competition/first_run/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "317375b6-3a44-47b1-8177-7bf272056690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/.local/bin/state\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/state/__main__.py\", line 68, in main\n",
      "    run_tx_infer(args)\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/state/_cli/_tx/_infer.py\", line 58, in run_tx_infer\n",
      "    cfg = load_config(config_path)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/.local/share/uv/tools/arc-state/lib/python3.11/site-packages/state/_cli/_tx/_infer.py\", line 51, in load_config\n",
      "    raise FileNotFoundError(f\"Could not find config file: {cfg_path}\")\n",
      "FileNotFoundError: Could not find config file: competition/first_run/config.yaml\n"
     ]
    }
   ],
   "source": [
    "! state tx infer \\\n",
    "  --output \"competition/prediction.h5ad\" \\\n",
    "  --model_dir \"competition/first_run\" \\\n",
    "  --checkpoint \"competition/first_run/checkpoints/final.ckpt\" \\\n",
    "  --adata \"competition_support_set/competition_val_template.h5ad\" \\\n",
    "  --pert_col \"target_gene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbacc1ae-bce3-4810-97b7-bf701923b391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842666c-70e1-4ed6-895c-8c32d7679d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70822381-05ad-4c1a-ab5e-37537d6505f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f7e9422-1315-4e80-97e4-65c964124597",
   "metadata": {},
   "source": [
    "# Run Cell-Eval on the resulting anndata and submit your entry to the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f446456a-a271-42d0-a291-78e03cedd559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install zstd for cell eval prep\n",
    "! sudo apt install -y zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be4638-5962-42c7-96f0-28c4c645efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tool run --from git+https://github.com/ArcInstitute/cell-eval@main cell-eval prep -i competition/prediction.h5ad -g competition_support_set/gene_names.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a05cb1f-22f7-44f6-b68e-17f663a17bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd73b98-3277-46d9-935c-a89650ba0bcb",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572457bb-1f11-41ae-9932-c6159363198a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bceab8-fe43-4416-ae73-69a8f2a24da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881192e-bfd6-4d5b-a411-5b20dfb1f59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f391274-2225-44fc-8f68-86879799c6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
